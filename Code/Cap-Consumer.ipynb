{"cells":[{"cell_type":"code","source":["# Run once then comment out\n%pip install confluent-kafka"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"19721196-3977-41c6-aa59-a956ab1b154d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Python interpreter will be restarted.\nRequirement already satisfied: confluent-kafka in /databricks/python3/lib/python3.9/site-packages (1.9.2)\nPython interpreter will be restarted.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Python interpreter will be restarted.\nRequirement already satisfied: confluent-kafka in /databricks/python3/lib/python3.9/site-packages (1.9.2)\nPython interpreter will be restarted.\n"]}}],"execution_count":0},{"cell_type":"code","source":["import confluent_kafka as kafka\nimport json\nfrom datetime import datetime, timedelta\nfrom pyspark.sql.functions import col"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"932d3830-445c-41ef-bed3-87ff875a238f","inputWidgets":{},"title":"Import Packages"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["props = {\n    # our class settings:\n    'sasl.username':'UGZQZXBMU4ITW4TE',\n    'sasl.password': 'vWTRzZdjZxnZu9NTpgBctijiq6lxNmMMcZ3tZejvVftLTvEjS2eTb1OSeKPVww4F',\n    'bootstrap.servers':'pkc-56d1g.eastus.azure.confluent.cloud:9092',\n    # boilerplate settings\n    'security.protocol':'SASL_SSL',\n    'sasl.mechanisms':'PLAIN',\n    'session.timeout.ms':45000,\n    'auto.offset.reset':'earliest',\n    'group.id':'python-group-25',\n}\n\nconsumer = kafka.Consumer(props)\nconsumer.subscribe(['chi-crime'])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"87d787b2-b65c-4e94-84ad-a825003d48f4","inputWidgets":{},"title":"Create Consumer"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["timeout_in_sec = 30\ntry:\n    messages = []\n    start_time = datetime.now()\n    while True:\n        msg = consumer.poll(5.0)\n        if msg is not None and msg.error() is None:\n            messages.append(msg)\n        if datetime.now() >= start_time + timedelta(seconds=timeout_in_sec):\n            raise Exception('Timeout')\nexcept Exception as e:\n    print(f'Exception: {str(e)[:100]}')\nfinally:\n    consumer.close()\n    print(f'{len(messages)} messages have been received!')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"f2793e94-e34e-4bf7-8426-23dd30f953a8","inputWidgets":{},"title":"Take in Kafka messages"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Exception: Timeout\n105 messages have been received!\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Exception: Timeout\n105 messages have been received!\n"]}}],"execution_count":0},{"cell_type":"code","source":["df_cols = list(json.loads(messages[0].value().decode('utf-8')).keys())\ndf_data = []\n\nfor i in messages:\n    data = list(json.loads(i.value().decode('utf-8')).values())\n    df_data.append(data)\n    \ndf = spark.createDataFrame(df_data, df_cols)\n# df.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"9cd2a70c-8c14-4078-b373-e4215176a21b","inputWidgets":{},"title":"Create DataFrame from messages"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = df.drop('y_coordinate', 'x_coordinate')\ndf = df.withColumn('id', df.id.cast('int'))\ndf = df.withColumn('beat', df.beat.cast('int'))\ndf = df.withColumn('district', df.district.cast('int'))\ndf = df.withColumn('date', df.date.cast('timestamp'))\ndf = df.withColumn('latitude', df.latitude.cast('float'))\ndf = df.withColumn('longitude', df.longitude.cast('float'))\n\ndf = df.dropDuplicates()\n\nprint(df.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"b13f3c0d-d0d5-4cfd-8290-fc17066da827","inputWidgets":{},"title":"Clean data"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"105\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["105\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col, min, max\n\nserver = 'cohort40-sql-2.database.windows.net'\ndatabase = 'test-ck'\nuser = 'testuser25'\npassword = 'T3STPASSWORD!!'\ntable = 'dbo.crime'\n\nmax_id = df.agg(max('ID')).collect()[0][0]\nmin_id = df.agg(min('ID')).collect()[0][0]\n\n# read in sql database\nquery = f\"(SELECT id FROM {table} WHERE ID BETWEEN {min_id} AND {max_id}) as t\"\njdbcDF = spark.read.format(\"jdbc\") \\\n    .option(\"url\", f\"jdbc:sqlserver://{server}:1433;databaseName={database};\") \\\n    .option(\"dbtable\", query) \\\n    .option(\"user\", user) \\\n    .option(\"password\", password) \\\n    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n    .load()\n\ndf = df.join(jdbcDF, 'ID', 'leftanti')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"19401363-a091-4a3b-95ef-c4d35f5274ea","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# add new rows to SQL database\ndf.write.format('jdbc').option(\n    'url', f'jdbc:sqlserver://{server}:1433;databaseName={database};'\n    ) \\\n    .mode('append') \\\n    .option('dbtable', table) \\\n    .option('user', user) \\\n    .option('password', password) \\\n    .option('driver', 'com.microsoft.sqlserver.jdbc.SQLServerDriver') \\\n    .save()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e43a2f3d-d868-45db-8d4c-3a3a56048eee","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2470460636152967>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# add new rows to SQL database\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjdbc\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124murl\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjdbc:sqlserver://\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mserver\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:1433;databaseName=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdatabase\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m;\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      4\u001B[0m     ) \\\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdbtable\u001B[39m\u001B[38;5;124m'\u001B[39m, table) \\\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m'\u001B[39m, user) \\\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpassword\u001B[39m\u001B[38;5;124m'\u001B[39m, password) \\\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdriver\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcom.microsoft.sqlserver.jdbc.SQLServerDriver\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;241m.\u001B[39msave()\n\n\u001B[0;31mNameError\u001B[0m: name 'df' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'df' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2470460636152967>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# add new rows to SQL database\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjdbc\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124murl\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjdbc:sqlserver://\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mserver\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:1433;databaseName=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdatabase\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m;\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      4\u001B[0m     ) \\\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdbtable\u001B[39m\u001B[38;5;124m'\u001B[39m, table) \\\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m'\u001B[39m, user) \\\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpassword\u001B[39m\u001B[38;5;124m'\u001B[39m, password) \\\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdriver\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcom.microsoft.sqlserver.jdbc.SQLServerDriver\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;241m.\u001B[39msave()\n\n\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Cap-Consumer","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":736618605178336}},"nbformat":4,"nbformat_minor":0}
